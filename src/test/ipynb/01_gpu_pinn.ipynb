{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************************************************************\n",
      "Author: Shota DEGUCHI\n",
      "        Strucural Analysis Lab. Kyushu Uni. (July 19, 2021)\n",
      "************************************************************\n",
      "\n",
      "current python version: 3.6.9 (default, Jan 26 2021, 15:33:00) \n",
      "[GCC 8.4.0]\n",
      "        tf     version: 2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "\"\"\"\n",
    "************************************************************\n",
    "Author: Shota DEGUCHI\n",
    "        Strucural Analysis Lab. Kyushu Uni. (July 19, 2021)\n",
    "************************************************************\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "print(\"current python version:\", sys.version)\n",
    "print(\"        tf     version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "################      HYPER PARAMETERS      ################\n",
    "############################################################\n",
    "\n",
    "lr      = 1e-3\n",
    "c_tol   = 1e-3\n",
    "n_btch  = 2 ** 7   # 2 ** x: 5 => 32, 6 => 64, 7 => 128, 8 => 256\n",
    "n_epch  = int(5e2)\n",
    "f_mntr  = int(1e1)\n",
    "layers  = [3] + 7 * [20] + [3]\n",
    "gpu_flg = 0   # 0, 1, 2, or 4 (see: https://www.tensorflow.org/guide/gpu)\n",
    "              # 0: Restrict TensorFlow to only use the first GPU\n",
    "              # 1: Currently, memory growth needs to be the same across GPUs\n",
    "              # 2: Create 2 virtual GPUs with 1GB memory each\n",
    "              # 4: Create 4 virtual GPUs with 1GB memory each\n",
    "\n",
    "seed    = 1234\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_star = pd.read_csv('../../../data/asymmetric_squares/U_arrange_data.csv', header = None, sep = '\\s+').values\n",
    "P_star = pd.read_csv('../../../data/asymmetric_squares/p_arrange_data.csv', header = None, sep = '\\s+').values\n",
    "t0, t1, dt =  0., 20,  .1; t_star = np.arange  (t0, t1, dt); t_star = t_star.reshape(-1, 1)\n",
    "x0, x1, nx =  1.,  8., 99; x_star = np.linspace(x0, x1, nx)\n",
    "y0, y1, ny = -2.,  2., 50; y_star = np.linspace(y0, y1, ny)\n",
    "x_star, y_star = np.meshgrid(x_star, y_star); x_star, y_star = x_star.reshape(-1, 1), y_star.reshape(-1, 1)\n",
    "X_star = np.c_[x_star, y_star]\n",
    "\n",
    "N = X_star.shape[0]; T = t_star.shape[0]\n",
    "XX = np.tile(X_star[:,0:1], (1, T)); YY = np.tile(X_star[:,1:2], (1, T)); TT = np.tile(t_star, (1, N)).T\n",
    "UU = U_star[:,0].reshape(T, N).T; VV = - U_star[:,1].reshape(T, N).T; PP = P_star.reshape(T, N).T\n",
    "\n",
    "ns_lvl = .20\n",
    "UU_ns = UU + ns_lvl * np.std(UU) * np.random.randn(UU.shape[0], UU.shape[1])\n",
    "VV_ns = VV + ns_lvl * np.std(VV) * np.random.randn(VV.shape[0], VV.shape[1])\n",
    "PP_ns = PP + ns_lvl * np.std(PP) * np.random.randn(PP.shape[0], PP.shape[1])\n",
    "\n",
    "x = XX.flatten()[:,None]; y = YY.flatten()[:,None]; t = TT.flatten()[:,None]\n",
    "u = UU.flatten()[:,None]; v = VV.flatten()[:,None]; p = PP.flatten()[:,None]\n",
    "u_ns = UU_ns.flatten()[:,None]; v_ns = VV_ns.flatten()[:,None]; p_ns = PP_ns.flatten()[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_trn: 69300\n",
      "N_val: 29700\n"
     ]
    }
   ],
   "source": [
    "N_trn_frac = .007\n",
    "N_val_frac = .003\n",
    "N_trn = int(N_trn_frac * N * T)\n",
    "N_val = int(N_val_frac * N * T)\n",
    "# N_trn = int(300)\n",
    "# N_val = int(300)\n",
    "\n",
    "N_all = N_trn + N_val\n",
    "\n",
    "idx_all = np.random.choice(N * T, N_all, replace = False)\n",
    "\n",
    "idx_trn = idx_all[0 : N_trn]\n",
    "idx_val = idx_all[N_trn : N_all]\n",
    "\n",
    "tm = 10; tm = np.array([tm])\n",
    "\n",
    "u0 = -.3; u1 = 1.5;  ut = .3\n",
    "v0 = -.6; v1 = .6 ;  vt = .3\n",
    "p0 = -.6; p1 = .2;   pt = .2\n",
    "\n",
    "idx_trn = np.random.choice(N * T, N_trn, replace = False)\n",
    "x_trn    = x[idx_trn,:];    y_trn    = y[idx_trn,:];    t_trn    = t[idx_trn,:]\n",
    "u_trn    = u[idx_trn,:];    v_trn    = v[idx_trn,:];    p_trn    = p[idx_trn,:]\n",
    "u_trn_ns = u_ns[idx_trn,:]; v_trn_ns = v_ns[idx_trn,:]; p_trn_ns = p_ns[idx_trn,:]\n",
    "\n",
    "idx_val = np.random.choice(N * T, N_val, replace = False)\n",
    "x_val    = x[idx_val,:];    y_val    = y[idx_val,:];    t_val    = t[idx_val,:]\n",
    "u_val    = u[idx_val,:];    v_val    = v[idx_val,:];    p_val    = p[idx_val,:]\n",
    "u_val_ns = u_ns[idx_val,:]; v_val_ns = v_ns[idx_val,:]; p_val_ns = p_ns[idx_val,:]\n",
    "\n",
    "print(\"N_trn:\", N_trn)\n",
    "print(\"N_val:\", N_val)\n",
    "\n",
    "# plt.scatter(x_trn, y_trn, marker = \".\")\n",
    "# plt.scatter(x_val, y_val, marker = \".\", alpha = .3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from datetime import datetime\n",
    "from tensorflow.summary import SummaryWriter\n",
    "\n",
    "class Shiba_PINN(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                x_trn, y_trn, t_trn, u_trn, v_trn, \n",
    "                x_val, y_val, t_val, u_val, v_val,  \n",
    "                w_init=\"Glorot\", h_num=40, out_num=2, \n",
    "                w_prd=1., w_pde=1., lr=1e-3, activ=\"tanh\", opt=\"Adam\"):\n",
    "        super(Shiba_PINN, self).__init__()\n",
    "\n",
    "        init_weight = self.weight_initializer(w_init)\n",
    "\n",
    "        self._dtype = tf.float32\n",
    "        self.activ = activ\n",
    "\n",
    "        # training set\n",
    "        self.x_trn = x_trn; self.y_trn = y_trn; self.t_trn = t_trn\n",
    "        self.u_trn = u_trn; self.v_trn = v_trn\n",
    "        \n",
    "        # validation set\n",
    "        self.x_val = x_val; self.y_val = y_val; self.t_val = t_val\n",
    "        self.u_val = u_val; self.v_val = v_val\n",
    "\n",
    "        self.lr = lr\n",
    "        self.optimizer = self.get_optimizer(opt) # 最適化手法\n",
    "\n",
    "        self.rhoi = tf.constant([1],   dtype = self._dtype)\n",
    "        self.nu   = tf.constant([.01], dtype = self._dtype)\n",
    "\n",
    "        # weight for loss terms\n",
    "        self.w_prd = w_prd\n",
    "        self.w_pde = w_pde\n",
    "\n",
    "        # self.inputs  = InputLayer(name='inputs', input_shape=(input_num,))                                        # 入力層は省略可\n",
    "        # self.layer1  = Dense(name='layer1' , kernel_initializer=\"glorot_normal\", bias_initializer='zeros', units=h_num  )\n",
    "        self.layer1  = Dense(name='layer1' , kernel_initializer=init_weight, bias_initializer='zeros', units=h_num  )\n",
    "        self.layer2  = Dense(name='layer2' , kernel_initializer=init_weight, bias_initializer='zeros', units=h_num  )\n",
    "        self.layer3  = Dense(name='layer3' , kernel_initializer=init_weight, bias_initializer='zeros', units=h_num  )\n",
    "        self.layer4  = Dense(name='layer4' , kernel_initializer=init_weight, bias_initializer='zeros', units=h_num  )\n",
    "        self.layer5  = Dense(name='layer5' , kernel_initializer=init_weight, bias_initializer='zeros', units=h_num  )\n",
    "        self.layer6  = Dense(name='layer6' , kernel_initializer=init_weight, bias_initializer='zeros', units=h_num  )\n",
    "        self.layer7  = Dense(name='layer7' , kernel_initializer=init_weight, bias_initializer='zeros', units=h_num  )\n",
    "        self.outputs = Dense(name='outputs', kernel_initializer=init_weight, bias_initializer='zeros', units=out_num)\n",
    "    \n",
    "    def weight_initializer(self, w_init):\n",
    "        if   w_init == \"Glorot\":\n",
    "            return tf.keras.initializers.GlorotNormal()\n",
    "        elif w_init == \"He\":\n",
    "            return  tf.keras.initializers.HeNormal()\n",
    "        elif w_init == \"LeCun\":\n",
    "            return tf.keras.initializers.LecunNormal()\n",
    "        else:\n",
    "            raise Exception(\">>>>> Exception: weight initializer not specified correctly\")\n",
    "\n",
    "    def get_activation(self, X):\n",
    "        if self.activ == \"tanh\":\n",
    "            return tf.keras.activations.tanh(X)\n",
    "        elif self.activ == \"gelu\":\n",
    "            return tf.keras.activations.gelu(X)\n",
    "        elif self.activ == \"silu\" or self.activ == \"swish-1\":\n",
    "            return tf.keras.activations.swish(X)\n",
    "        else:\n",
    "            raise Exception(\">>>>> Exception: activation function not specified correctly\")\n",
    "    \n",
    "    def get_optimizer(self, opt_name):\n",
    "        if   opt_name == \"SGD\":\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate = self.lr, momentum = 0.0, nesterov = False)\n",
    "        elif opt_name == \"Adadelta\":\n",
    "            optimizer = tf.keras.optimizers.Adadelta(learning_rate = self.lr, rho = 0.95)\n",
    "        elif opt_name == \"Adagrad\":\n",
    "            optimizer = tf.keras.optimizers.Adagrad(learning_rate = self.lr, initial_accumulator_value = 0.1)\n",
    "        elif opt_name == \"RMSprop\":\n",
    "            optimizer = tf.keras.optimizers.RMSprop(learning_rate = self.lr, rho = 0.9, momentum = 0.0, centered = False)\n",
    "        elif opt_name == \"Adam\":\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate = self.lr, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "        elif opt_name == \"Adamax\":\n",
    "            optimizer = tf.keras.optimizers.Adamax(learning_rate = self.lr, beta_1 = 0.9, beta_2 = 0.999)\n",
    "        elif opt_name == \"Nadam\":\n",
    "            optimizer = tf.keras.optimizers.Nadam(learning_rate = self.lr, beta_1 = 0.9, beta_2 = 0.999)\n",
    "        else:\n",
    "            raise Exception(\">>>>> Exception: optimizer not specified correctly\")\n",
    "            \n",
    "        return optimizer\n",
    "\n",
    "    # @tf.function\n",
    "    def call(self, X):\n",
    "        # X = self.inputs(X)                        #入力層は省略可 \n",
    "        X = self.get_activation(self.layer1(X))\n",
    "        X = self.get_activation(self.layer2(X))\n",
    "        X = self.get_activation(self.layer3(X))\n",
    "        X = self.get_activation(self.layer4(X))\n",
    "        X = self.get_activation(self.layer5(X))\n",
    "        X = self.get_activation(self.layer6(X))\n",
    "        X = self.get_activation(self.layer7(X))\n",
    "        out = self.get_activation(self.outputs(X))\n",
    "        return out\n",
    "\n",
    "    def PDE(self, x, y, t):\n",
    "        rhoi = self.rhoi\n",
    "        nu   = self.nu\n",
    "        x = tf.convert_to_tensor(x, dtype = self._dtype)\n",
    "        y = tf.convert_to_tensor(y, dtype = self._dtype)\n",
    "        t = tf.convert_to_tensor(t, dtype = self._dtype)\n",
    "\n",
    "        # print(x)\n",
    "        with tf.GradientTape(persistent = True) as tp1:\n",
    "            tp1.watch(x)\n",
    "            tp1.watch(y)\n",
    "            tp1.watch(t)\n",
    "\n",
    "            u_v_p = self.call(tf.concat([x, y, t], 1))\n",
    "            u = u_v_p[:,0:1]\n",
    "            v = u_v_p[:,1:2]\n",
    "            p = u_v_p[:,2:3]\n",
    "            \n",
    "            u_x = tp1.gradient(u, x); u_y = tp1.gradient(u, y)\n",
    "            v_x = tp1.gradient(v, x); v_y = tp1.gradient(v, y)\n",
    "        \n",
    "        u_t  = tp1.gradient(u, t)  ; v_t = tp1.gradient(v, t)\n",
    "        p_x  = tp1.gradient(p, x)  ; p_y = tp1.gradient(p, y)\n",
    "        u_xx = tp1.gradient(u_x, x); u_yy = tp1.gradient(u_y, y)\n",
    "        v_xx = tp1.gradient(v_x, x); v_yy = tp1.gradient(v_y, y)\n",
    "        # del tp1\n",
    "        gv_c = u_x + v_y                                                   # continuity\n",
    "        gv_x = u_t + u * u_x + v * u_y + rhoi * p_x - nu * (u_xx + u_yy)   # momentum\n",
    "        gv_y = v_t + u * v_x + v * v_y + rhoi * p_y - nu * (v_xx + v_yy)\n",
    "        return u, v, p, gv_c, gv_x, gv_y\n",
    "\n",
    "    def loss_glb(self, x, y, t, u, v):\n",
    "        u_hat, v_hat, p_hat, gv_c_hat, gv_x_hat, gv_y_hat = self.PDE(x, y, t)\n",
    "        loss_prd =    tf.reduce_mean(tf.square(u - u_hat)) \\\n",
    "                    + tf.reduce_mean(tf.square(v - v_hat))\n",
    "        loss_prd = self.w_prd * loss_prd\n",
    "        loss_pde =    tf.reduce_mean(tf.square(gv_c_hat)) \\\n",
    "                    + tf.reduce_mean(tf.square(gv_x_hat)) \\\n",
    "                    + tf.reduce_mean(tf.square(gv_y_hat))\n",
    "        loss_pde = self.w_pde * loss_pde\n",
    "        loss_glb = loss_prd + loss_pde\n",
    "        return loss_glb\n",
    "    \n",
    "    @tf.function\n",
    "    def loss_grad(self, x, y, t, u, v):\n",
    "        with tf.GradientTape() as tp:\n",
    "            loss = self.loss_glb(x, y, t, u, v)\n",
    "        grad = tp.gradient(loss, self.trainable_variables)\n",
    "        return loss, grad\n",
    "\n",
    "    # @tf.function\n",
    "    def minibatch_train(self, batch):\n",
    "        n_data  = self.x_trn.shape[0]\n",
    "        sff_idx = np.random.permutation(n_data)\n",
    "        for idx in range(0, n_data, batch):\n",
    "            x_trn_batch = self.x_trn[sff_idx[idx: idx + batch if idx + batch < n_data else n_data]]\n",
    "            y_trn_batch = self.y_trn[sff_idx[idx: idx + batch if idx + batch < n_data else n_data]]\n",
    "            t_trn_batch = self.t_trn[sff_idx[idx: idx + batch if idx + batch < n_data else n_data]]\n",
    "            u_trn_batch = self.u_trn[sff_idx[idx: idx + batch if idx + batch < n_data else n_data]]\n",
    "            v_trn_batch = self.v_trn[sff_idx[idx: idx + batch if idx + batch < n_data else n_data]]\n",
    "            x_trn_batch = tf.convert_to_tensor(x_trn_batch, dtype = self._dtype)\n",
    "            y_trn_batch = tf.convert_to_tensor(y_trn_batch, dtype = self._dtype)\n",
    "            t_trn_batch = tf.convert_to_tensor(t_trn_batch, dtype = self._dtype)\n",
    "            u_trn_batch = tf.convert_to_tensor(u_trn_batch, dtype = self._dtype)\n",
    "            v_trn_batch = tf.convert_to_tensor(v_trn_batch, dtype = self._dtype)\n",
    "            loss_trn, grad_trn = self.loss_grad(x_trn_batch, y_trn_batch, t_trn_batch, u_trn_batch, v_trn_batch)\n",
    "            self.optimizer.apply_gradients(zip(grad_trn, self.trainable_variables))\n",
    "\n",
    "        return loss_trn\n",
    "    \n",
    "    def fullbatch_train(self):\n",
    "        x_trn = tf.convert_to_tensor(self.x_trn, dtype = self._dtype)\n",
    "        y_trn = tf.convert_to_tensor(self.y_trn, dtype = self._dtype)\n",
    "        t_trn = tf.convert_to_tensor(self.t_trn, dtype = self._dtype)\n",
    "        u_trn = tf.convert_to_tensor(self.u_trn, dtype = self._dtype)\n",
    "        v_trn = tf.convert_to_tensor(self.v_trn, dtype = self._dtype)\n",
    "        loss_trn, grad_trn = self.loss_grad(x_trn, y_trn, t_trn, u_trn, v_trn)\n",
    "        self.optimizer.apply_gradients(zip(grad_trn, self.trainable_variables))\n",
    "        return loss_trn\n",
    "\n",
    "    def train(self, epoch = 10 ** 5, batch = 693, tol = 1e-5): \n",
    "        # logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        t0 = time.time()\n",
    "        # with tf.profiler.experimental.Profile(logs):\n",
    "        for ep in range(epoch):\n",
    "\n",
    "            # minibatch 学習\n",
    "            ep_loss_trn = self.minibatch_train(batch)\n",
    "            # fullibatch 学習\n",
    "            # ep_loss_trn = self.fullbatch_train()\n",
    "\n",
    "            if ep % f_mntr == 0:\n",
    "                elps = time.time() - t0\n",
    "                print(\"ep: %d, loss_trn: %.6e, elps: %.3f\" % (ep, ep_loss_trn, elps))\n",
    "                t0 = time.time()\n",
    "                \n",
    "            if ep_loss_trn < tol:\n",
    "                print(\">>>>> program terminating with the loss converging to its tolerance.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shiba_PINN = Shiba_PINN(x_trn, y_trn, t_trn, u_trn, v_trn, \n",
    "               x_val, y_val, t_val, u_val, v_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate  : 0.001\n",
      "# of epoch     : 500\n",
      "batch size     : 128\n",
      "convergence tol: 0.001\n",
      "N_trn          : 69300\n",
      "N_val          : 29700\n",
      "ep: 0, loss_trn: 2.523114e-01, elps: 1.042\n",
      "ep: 10, loss_trn: 2.326749e-01, elps: 4.022\n",
      "ep: 20, loss_trn: 2.244175e-01, elps: 4.018\n",
      "ep: 30, loss_trn: 2.199011e-01, elps: 4.065\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0eb1ba391c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CPU:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mShiba_PINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_tol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-a68116947280>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch, batch, tol)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;31m# ep_loss_trn = self.minibatch_train(batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# fullibatch 学習\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mep_loss_trn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfullbatch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mf_mntr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-a68116947280>\u001b[0m in \u001b[0;36mfullbatch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mu_trn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mv_trn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mloss_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_trn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_trn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/shibata/05_tf2/.venv_tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/shibata/05_tf2/.venv_tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/shibata/05_tf2/.venv_tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/shibata/05_tf2/.venv_tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/shibata/05_tf2/.venv_tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Documents/shibata/05_tf2/.venv_tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"learning rate  :\", lr)\n",
    "print(\"# of epoch     :\", n_epch)\n",
    "print(\"batch size     :\", n_btch)\n",
    "print(\"convergence tol:\", c_tol)\n",
    "print(\"N_trn          :\", N_trn)\n",
    "print(\"N_val          :\", N_val)\n",
    "\n",
    "with tf.device(\"GPU:0\"):\n",
    "    Shiba_PINN.train(epoch = n_epch, tol = c_tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"shiba_pinn_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "layer1 (Dense)               multiple                  80        \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               multiple                  420       \n",
      "_________________________________________________________________\n",
      "layer3 (Dense)               multiple                  420       \n",
      "_________________________________________________________________\n",
      "layer4 (Dense)               multiple                  420       \n",
      "_________________________________________________________________\n",
      "layer5 (Dense)               multiple                  420       \n",
      "_________________________________________________________________\n",
      "layer6 (Dense)               multiple                  420       \n",
      "_________________________________________________________________\n",
      "layer7 (Dense)               multiple                  420       \n",
      "_________________________________________________________________\n",
      "outputs (Dense)              multiple                  42        \n",
      "=================================================================\n",
      "Total params: 2,642\n",
      "Trainable params: 2,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Shiba_PINN(tf.zeros([100,3]))\n",
    "Shiba_PINN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b616386e80455efdbd8f94f85b4166ca580f103d0731817a6e6174a39cdaa738"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('.venv_tf2': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
